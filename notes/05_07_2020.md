# 5 July 2020

## Twice Differentiable functions in neural networks

In numerical optimizations, I learned that Newton/Quasi-Newton methods were much faster that steepest descent, given the tradeoff of cost and requirement that the objective function is twice differentiable, or at least has a well-conditioned Hessian.

### Matern Covariance (kernel)

$$
C_{v}(d) = \sigma^{2} \frac{2^{1-v}}{\Gamma(v)} (\sqrt{2v} \frac{d}{\rho})^{v} K_{v} (\sqrt{2v} \frac{d}{\rho})
$$

A function measuring the covariance between two points (statistics), and is $$v-1$$ times differentiable.

### Hessian Sub-Sampling

Stochastic methods, like stochastic gradient descent (SGD) are chosen over deterministic methods because they are cheaper to compute per iteration, however are slower to converge. Second order methods, like Newton's method, have quicker local convergence rates with a high computation cost per iteration. This leads to gradient and Hessian sub-sampling methods. For 

$$ \forall x \in \mathbb{R}^{P}, P \gg 1$$

$$
g(x) := \frac{1}{|S_{g}|}\sum_{j\in{S_{g}}}\nabla{f_{j}}(x)
$$

$$
H(x) := \frac{1}{|S_{H}|} \sum_{j \in {S_{H}}} \nabla^{2}f_{j}(x)
$$

Where $$S_g$$ and $$S_H$$ describe the sampled set. 

### Large Scale Distributed Deep networks Dean et al.

### Second-Order Optimization for Non-Convex Machine Learning: An Empirical Study Xu et al.

* First order methods (e.g. SGD) have deficiencies like slow convergence, sensitivity to hyperparameters (e.g. learning rate), and careful attention to escape saddle points (not robust).
* Second order methods incorporate curvature information, and can solve the first order deficiencies.

* Are there networks that already implement second order optimization?
* The experiements for the deep autoencoder consistently show SGD begins to decrease loss sooner, although at a slower rate; versus TR where the convergence rate is much higher and stable, although at a later point.

#### * I would like to see an experiment that starts with a first order method like SGD and then continue with a second order method

* In a large network, how would one determine when to make the switch? This is another hyperparameter.
* [This tutorial](https://towardsdatascience.com/custom-optimizer-in-tensorflow-d5b41f75644a) shows how to implement a custom optimizer in Tensorflow. One could try to implement a second order optimizer, and run a training loop that switches optimizers after a number of iterations using a first order optimizer. It is still very unclear to me how stochastic gradient descent or sampling is handled; I looked through the source code and only found traces of how the task is divided into multiple devices (GPUs).
* This [code](https://gist.github.com/guillaume-chevalier/6b01c4e43a123abf8db69fa97532993f) is an example of computing the Hessian, albeit on 1-dimmensional data, for a second-order optimization method.
* The question then becomes, how can we acquire the Hessian in the scope of the custom Tensorflow optimizer? It seems that the gradients are passed in, but the Hessian is not. Would one have to override the gradient computation as well?
